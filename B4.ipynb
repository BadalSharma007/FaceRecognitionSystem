{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tf_keras.utils import to_categorical\n",
    "\n",
    "# Download the full LFW dataset with all people included (no filtering)\n",
    "lfw_people=fetch_lfw_people(min_faces_per_person=0,resize=0.4,color=False)\n",
    "\n",
    "# Get the face images and labels\n",
    "data=lfw_people.images # face images\n",
    "labels=lfw_people.target # people\n",
    "target_names= lfw_people.target_names #list of all person names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 13233\n",
      "Image size : 50x37\n",
      "Number of classes (people) : 5749\n"
     ]
    }
   ],
   "source": [
    "#Print dataset details\n",
    "print(f\"Total images: {data.shape[0]}\")\n",
    "print(f\"Image size : {data.shape[1]}x{data.shape[2]}\") # heightxwidth\n",
    "print(f\"Number of classes (people) : {len(target_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape : (10586, 50, 37, 1)\n",
      "Testing data shape: (2647, 50, 37, 1)\n"
     ]
    }
   ],
   "source": [
    "#Reshape the data to include the channel dimension (for grayscale images, channel 1)\n",
    "data = data.reshape((data.shape[0], data.shape[1],data.shape[2],1))\n",
    "#split tha data\n",
    "x_train,x_test, y_train,y_test= train_test_split(data,labels,test_size=0.2,random_state=42)\n",
    "\n",
    "print(f\"Training data shape : {x_train.shape}\")\n",
    "print(f\"Testing data shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tf_keras as tf\n",
    "from tf_keras import layers, Model\n",
    "\n",
    "def mb_conv_block(x,inputs,filters,kernel_size,strides,expand_ratio,se_ratio):\n",
    "    in_channels=x.shape[-1] # Number of inputs filters\n",
    "    expanded_filter=in_channels * expand_ratio\n",
    "    \n",
    "    #Expension phase\n",
    "    if expand_ratio != 1:\n",
    "        x =layers.Conv2D(expanded_filter,kernel_size=1, padding='same',use_bias=False)(x)\n",
    "        x=layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "    else:\n",
    "        x=x\n",
    "        \n",
    "    #Depthwise convolution\n",
    "    x=layers.DepthwiseConv2D(kernel_size,strides=strides,padding='same',use_bias=False)(x)\n",
    "    x=layers.BatchNormalization()(x)\n",
    "    x=layers.ReLU()(x)\n",
    "    \n",
    "    # Squeeze and Excitation block\n",
    "    # if se_ratio:\n",
    "    #     se_shape=(1,1,in_channels * expand_ratio)\n",
    "    #     se=layers.GlobalAveragePooling2D()(x)\n",
    "    #     se=layers.Reshape(se_shape)(x)\n",
    "    #     se=layers.Dense(se_shape[-1]//4,activation='relu')(se)\n",
    "    #     se=layers.Dense(se_shape[-1],activation='sigmoid')(se)\n",
    "    #     x=layers.multiply([x,se])\n",
    "    \n",
    "    \n",
    "    if se_ratio:\n",
    "        in_channels = x.shape[-1]\n",
    "        se_shape = (1, 1, in_channels)\n",
    "        se = layers.GlobalAveragePooling2D()(x)  # This should output shape (None, in_channels)\n",
    "        se = layers.Reshape(se_shape)(se)  # Change this line to match the output shape\n",
    "        se = layers.Dense(in_channels // 4, activation='relu')(se)\n",
    "        se = layers.Dense(in_channels, activation='sigmoid')(se)\n",
    "        x = layers.multiply([x, se])  # Apply the squeeze-and-excitation weights\n",
    "\n",
    "        \n",
    "    #Output phase\n",
    "    x=layers.Conv2D(filters,kernel_size=1,padding='same',use_bias=False)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    \n",
    "    if in_channels ==filters and strides ==1:\n",
    "        x=layers.add([x,inputs])\n",
    "        \n",
    "    return x          \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EfficientNetB4(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial Conv layer\n",
    "    x = layers.Conv2D(48, kernel_size=3, strides=2, padding='same', use_bias=False)(inputs)  # Adjusted filters for B4\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    \n",
    "    # MBConv Blocks following EfficientNetB4 architecture scaling\n",
    "    x = mb_conv_block(x, inputs=x, filters=32, kernel_size=3, strides=1, expand_ratio=1, se_ratio=0.25)\n",
    "    x = mb_conv_block(x, inputs=x, filters=32, kernel_size=3, strides=2, expand_ratio=6, se_ratio=0.25)\n",
    "    x = mb_conv_block(x, inputs=x, filters=48, kernel_size=3, strides=2, expand_ratio=6, se_ratio=0.25)\n",
    "    x = mb_conv_block(x, inputs=x, filters=48, kernel_size=3, strides=1, expand_ratio=6, se_ratio=0.25)\n",
    "    x = mb_conv_block(x, inputs=x, filters=80, kernel_size=5, strides=2, expand_ratio=6, se_ratio=0.25)\n",
    "    x = mb_conv_block(x, inputs=x, filters=80, kernel_size=5, strides=1, expand_ratio=6, se_ratio=0.25)\n",
    "    x = mb_conv_block(x, inputs=x, filters=112, kernel_size=5, strides=1, expand_ratio=6, se_ratio=0.25)  # Adjusted for B4\n",
    "    x = mb_conv_block(x, inputs=x, filters=112, kernel_size=5, strides=1, expand_ratio=6, se_ratio=0.25)  # Adjusted for B4\n",
    "    x = mb_conv_block(x, inputs=x, filters=192, kernel_size=3, strides=2, expand_ratio=6, se_ratio=0.25)  # Adjusted for B4\n",
    "    x = mb_conv_block(x, inputs=x, filters=192, kernel_size=3, strides=1, expand_ratio=6, se_ratio=0.25)  # Adjusted for B4\n",
    "    x = mb_conv_block(x, inputs=x, filters=320, kernel_size=3, strides=1, expand_ratio=6, se_ratio=0.25)  # Adjusted for B4\n",
    "    \n",
    "    # Final layers\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)  # Adding dropout\n",
    "    x = layers.Dense(1280, activation='relu')(x)  # Adjusted units for B4\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model_4 = Model(inputs, outputs)\n",
    "    return model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (50, 37, 1)  # Set input shape to match the data\n",
    "model_3 = EfficientNetB4(input_shape=input_shape, num_classes=len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "331/331 [==============================] - 25s 68ms/step - loss: 8.1304 - accuracy: 0.0402 - val_loss: 9.2284 - val_accuracy: 0.0366\n",
      "Epoch 2/5\n",
      "331/331 [==============================] - 22s 65ms/step - loss: 7.6596 - accuracy: 0.0409 - val_loss: 8.7223 - val_accuracy: 0.0366\n",
      "Epoch 3/5\n",
      "331/331 [==============================] - 21s 65ms/step - loss: 7.4106 - accuracy: 0.0419 - val_loss: 9.7633 - val_accuracy: 0.0397\n",
      "Epoch 4/5\n",
      "331/331 [==============================] - 21s 65ms/step - loss: 7.2649 - accuracy: 0.0439 - val_loss: 9.2522 - val_accuracy: 0.0404\n",
      "Epoch 5/5\n",
      "331/331 [==============================] - 22s 65ms/step - loss: 7.1471 - accuracy: 0.0458 - val_loss: 8.7158 - val_accuracy: 0.0400\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the resized data\n",
    "history_3 = model_3.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 1s 15ms/step - loss: 8.7158 - accuracy: 0.0400\n",
      "Test accuracy: 4.00%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_3.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
